{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOPAPCS6VnOuXMccxYUG7nM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepukr007/fashion-MNIST/blob/main/fashion_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jczUzsfi6cdO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "JwfVn_d27vlw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = torchvision.datasets.FashionMNIST(\"./data\", download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "test_set = torchvision.datasets.FashionMNIST(\"./data\", download=True, train=False, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=50)\n",
        "test_loader = torch.utils.data.DataLoader(test_set,batch_size=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2Oh6AMxBqyD",
        "outputId": "4c163531-961f-4986-9453-e1126b22cc9a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 16133025.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 272747.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 4992877.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 25828082.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#printing out sizes\n",
        "\n",
        "print(\"Train and test set lenghts: \")\n",
        "print(len(train_set))\n",
        "print(len(test_set))\n",
        "print()\n",
        "print(\"Batch size\")\n",
        "print(next(iter(train_loader))[0].size())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRrmFedNMV5O",
        "outputId": "3fae53a6-d6a4-42d8-f514-97c9922c8cfa"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train and test set lenghts: \n",
            "60000\n",
            "10000\n",
            "\n",
            "Batch size\n",
            "torch.Size([50, 1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(net, self).__init__()\n",
        "\n",
        "        self.cbrm1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.cbrm2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=128*6*6, out_features=256)\n",
        "        self.fc2 = nn.Linear(in_features=256, out_features=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.cbrm1(x)\n",
        "        out = self.cbrm2(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        out = nn.functional.relu(out)\n",
        "        out = self.fc2(out)\n",
        "\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "2MO0RYz2NkXK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = net()\n",
        "model.to(device)\n",
        "\n",
        "error = nn.CrossEntropyLoss()\n",
        "\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsgHCw68OJcB",
        "outputId": "10a72644-b03f-4cf2-c44c-b19284d14778"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "net(\n",
            "  (cbrm1): Sequential(\n",
            "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (cbrm2): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc1): Linear(in_features=4608, out_features=256, bias=True)\n",
            "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Network Training\n",
        "\n",
        "num_epochs = 10\n",
        "count = 0\n",
        "\n",
        "\n",
        "# Accuracy - Classwise\n",
        "predictions_list = []\n",
        "labels_list = []\n",
        "\n",
        "acuuracy =0\n",
        "running_loss =0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        train = images.view(50, 1, 28, 28)\n",
        "        labels = labels\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(train)\n",
        "        loss = error(outputs, labels)\n",
        "\n",
        "        # make gradients zero\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #Back Propagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        count += 1\n",
        "        running_loss += loss.item()\n",
        "\n",
        "\n",
        "        if not (count % 500):\n",
        "          correct = 0\n",
        "          total = 0\n",
        "          for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            labels_list.append(labels)\n",
        "            test = images.view(50, 1, 28, 28)\n",
        "            outputs = model(test)\n",
        "            predictions = torch.max(outputs, 1)[1].to(device)\n",
        "            predictions_list.append(predictions)\n",
        "            correct += (predictions == labels).sum()\n",
        "            total += len(labels)\n",
        "            accuracy = correct * 100 / total\n",
        "            print(\"Iteration: {}, Loss: {}, Accuracy: {}%\".format(count, running_loss/500, accuracy))\n",
        "            running_loss = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ys94jwWgSuXr",
        "outputId": "201aa893-503f-4584-8d7e-ac7f5ad0ee93"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 86.0%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.0%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.0%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 93.0%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 93.20000457763672%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 93.33333587646484%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.85713958740234%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.25%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.22222137451172%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.80000305175781%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.18181610107422%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.16667175292969%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.30769348144531%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.0%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.0%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.125%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.4705810546875%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.44445037841797%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.42105102539062%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.00000762939453%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.0952377319336%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.0%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.91304779052734%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.75%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.5999984741211%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.61538696289062%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.70370483398438%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.71428680419922%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.7241439819336%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.73332977294922%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.80645751953125%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.875%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.75757598876953%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.76470184326172%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.94285583496094%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.11111450195312%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.21621704101562%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.26315307617188%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.35897064208984%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.50000762939453%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.48780822753906%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.52381134033203%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.55814361572266%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.59091186523438%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.71111297607422%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.6956558227539%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.63829803466797%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.625%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.69387817382812%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.5199966430664%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.4313735961914%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.38461303710938%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.45282745361328%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.48148345947266%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.54545593261719%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.60713958740234%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.56140899658203%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.55172729492188%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.44068145751953%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.36666870117188%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.26229858398438%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.29032897949219%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.31745910644531%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.3125%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.27692413330078%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.24242401123047%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.14925384521484%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.14705657958984%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.20289611816406%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.0%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.00000762939453%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.97222137451172%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.89041137695312%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.8648681640625%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.8933334350586%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.9473648071289%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.79220581054688%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.79486846923828%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.77215576171875%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.80000305175781%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.82716369628906%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.8536605834961%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.73493957519531%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.61904907226562%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.64706420898438%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.6279067993164%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.6551742553711%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.7272720336914%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.77528381347656%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.80000305175781%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.82418060302734%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.82608795166016%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.87096405029297%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.80850982666016%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.83157348632812%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.83333587646484%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.79381561279297%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.7755126953125%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.75757598876953%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.81999969482422%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.82177734375%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.82352447509766%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.78640747070312%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.75%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.77143096923828%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.75471496582031%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.75701141357422%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.81481170654297%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.8348617553711%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.83636474609375%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.78378295898438%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.73213958740234%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.75221252441406%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.73684692382812%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.73912811279297%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.77586364746094%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.77777862548828%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.7966079711914%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.84873962402344%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.81666564941406%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.76859283447266%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.80327606201172%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.8211441040039%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.83871459960938%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.80799865722656%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.80951690673828%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.85826873779297%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.84375%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.84495544433594%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.87692260742188%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.89312744140625%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.8484878540039%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.86466217041016%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.8358154296875%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.85185241699219%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.86764526367188%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.85401153564453%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.85507202148438%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.82733154296875%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.79999542236328%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.80142211914062%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.84507751464844%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.87413024902344%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.86111450195312%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.82069396972656%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.84931182861328%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.86394500732422%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.8648681640625%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.86577606201172%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.8933334350586%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.89403533935547%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.89472961425781%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.93463897705078%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.9610366821289%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.0%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.97435760498047%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.98725891113281%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.98734283447266%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.0%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.97500610351562%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.9378890991211%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.93827056884766%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.950927734375%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.9756088256836%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.0%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.01204681396484%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.03592681884766%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.04761505126953%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.08283996582031%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.0941162109375%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.10526275634766%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.11627960205078%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.09248352050781%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.04597473144531%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.02285766601562%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.03408813476562%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.03389739990234%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.03370666503906%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.01117706298828%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.0111083984375%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.02210235595703%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.02198028564453%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.02185821533203%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.01087188720703%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.97837829589844%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.9784927368164%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 91.97860717773438%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.0%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.02116394042969%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.03157806396484%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.02094268798828%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.03125%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.05181884765625%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.05154418945312%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.07179260253906%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.06122589111328%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.0710678100586%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.09091186523438%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.0904541015625%\n",
            "Iteration: 500, Loss: 0.016475956467420474, Accuracy: 92.06999969482422%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 86.0%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.0%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 92.0%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 92.5%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 92.80000305175781%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 93.0%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 92.57142639160156%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 92.25%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 92.22222137451172%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.80000305175781%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.63636779785156%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.66667175292969%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.38461303710938%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.14285278320312%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.33333587646484%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.5%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.76470184326172%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.8888931274414%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.99999237060547%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.70000457763672%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.9047622680664%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.90908813476562%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 92.0%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.91667175292969%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.83999633789062%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.84615325927734%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.77777862548828%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.85713958740234%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.93103790283203%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.79999542236328%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.54839324951172%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.625%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.63636779785156%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.64705657958984%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.65714263916016%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.77777862548828%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.72972869873047%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.7368392944336%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.69230651855469%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.6500015258789%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.65853881835938%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.66666412353516%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.72093200683594%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.7272720336914%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.86666870117188%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.86956787109375%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.7872314453125%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.70833587646484%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.71428680419922%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.5999984741211%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.49019622802734%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.34615325927734%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.39622497558594%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.40740966796875%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.45454406738281%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.46428680419922%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.33333587646484%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.24137878417969%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 91.01695251464844%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.86666870117188%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.7868881225586%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.80645751953125%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.85713958740234%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.84375%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.79999542236328%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.7272720336914%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.62686157226562%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.61764526367188%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.66666412353516%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.5142822265625%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.53521728515625%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.55555725097656%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.54794311523438%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.51351928710938%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.4800033569336%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.52631378173828%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.41558074951172%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.38461303710938%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.40506744384766%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.45000457763672%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.49382781982422%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.43902587890625%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.43373107910156%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.35713958740234%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.42353057861328%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.39534759521484%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.45977020263672%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.4772720336914%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.4494400024414%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.46666717529297%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.5054931640625%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.56521606445312%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.6236572265625%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.574462890625%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.56842041015625%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.64583587646484%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.63917541503906%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.63265228271484%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.62626647949219%\n",
            "Iteration: 1000, Loss: 0.03386755046051621, Accuracy: 90.65999603271484%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-fa163abc4cb6>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mlabels_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mpredictions_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-6ac5712576b9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbrm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbrm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \"\"\"\n\u001b[0;32m--> 171\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2448\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2450\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2451\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2452\u001b[0m     )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "for images, labels in test_loader:\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      labels_list.append(labels)\n",
        "      test = images.view(50, 1, 28, 28)\n",
        "      outputs = model(test)\n",
        "      predictions = torch.max(outputs, 1)[1].to(device)\n",
        "      predictions_list.append(predictions)\n",
        "      correct += (predictions == labels).sum()\n",
        "\n",
        "      total += len(labels)\n",
        "\n",
        "      accuracy = correct * 100 / total\n"
      ],
      "metadata": {
        "id": "hWyWcIWRTLFZ"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HhBPtjvfj-d0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J622bDJ4h2g6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}